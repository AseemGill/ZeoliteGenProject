{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (division, print_function)\n",
    "# import time\n",
    "\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy import stats\n",
    "# from scipy.spatial import distance\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from arg_helper import *\n",
    "from model import *\n",
    "# from args import *\n",
    "# from data import *\n",
    "# from data_parallel import *\n",
    "from evaluation import *\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "# Set seeds\n",
    "random.seed(10)\n",
    "\n",
    "def bfs_seq(G, start_id):\n",
    "    '''\n",
    "    get a bfs node sequence\n",
    "    :param G:\n",
    "    :param start_id:\n",
    "    :return:\n",
    "    '''\n",
    "    dictionary = dict(nx.bfs_successors(G, start_id))\n",
    "    start = [start_id]\n",
    "    output = [start_id]\n",
    "    while len(start) > 0:\n",
    "        next = []\n",
    "        while len(start) > 0:\n",
    "            current = start.pop(0)\n",
    "            neighbor = dictionary.get(current)\n",
    "            if neighbor is not None:\n",
    "                #### a wrong example, should not permute here!\n",
    "                # shuffle(neighbor)\n",
    "                next = next + neighbor\n",
    "        output = output + next\n",
    "        start = next\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def encode_adj(adj, max_prev_node=10, is_full = False):\n",
    "    '''\n",
    "\n",
    "    :param adj: n*n, rows means time step, while columns are input dimension\n",
    "    :param max_degree: we want to keep row number, but truncate column numbers\n",
    "    :return:\n",
    "    '''\n",
    "    if is_full:\n",
    "        max_prev_node = adj.shape[0]-1\n",
    "\n",
    "    # pick up lower tri\n",
    "    adj = np.tril(adj, k=-1)\n",
    "    n = adj.shape[0]\n",
    "    adj = adj[1:n, 0:n-1]\n",
    "\n",
    "    # use max_prev_node to truncate\n",
    "    # note: now adj is a (n-1)*(n-1) matrix\n",
    "    adj_output = np.zeros((adj.shape[0], max_prev_node))\n",
    "    for i in range(adj.shape[0]):\n",
    "        input_start = max(0, i - max_prev_node + 1)\n",
    "        input_end = i + 1\n",
    "        output_start = max_prev_node + input_start - input_end\n",
    "        output_end = max_prev_node\n",
    "        adj_output[i, output_start:output_end] = adj[i, input_start:input_end]\n",
    "        adj_output[i,:] = adj_output[i,:][::-1] # reverse order\n",
    "\n",
    "    return adj_output\n",
    "\n",
    "def encode_adj_flexible(adj):\n",
    "    '''\n",
    "    return a flexible length of output\n",
    "    note that here there is no loss when encoding/decoding an adj matrix\n",
    "    :param adj: adj matrix\n",
    "    :return:\n",
    "    '''\n",
    "    # pick up lower tri\n",
    "    adj = np.tril(adj, k=-1)\n",
    "    n = adj.shape[0]\n",
    "    adj = adj[1:n, 0:n-1]\n",
    "\n",
    "    adj_output = []\n",
    "    input_start = 0\n",
    "    for i in range(adj.shape[0]):\n",
    "        input_end = i + 1\n",
    "        adj_slice = adj[i, input_start:input_end]\n",
    "        adj_output.append(adj_slice)\n",
    "        non_zero = np.nonzero(adj_slice)[0]\n",
    "        input_start = input_end-len(adj_slice)+np.amin(non_zero)\n",
    "\n",
    "    return adj_output\n",
    "\n",
    "\n",
    "########## use pytorch dataloader\n",
    "class Graph_sequence_sampler_pytorch(torch.utils.data.Dataset):\n",
    "    def __init__(self, G_list, max_num_node=None, max_prev_node=None, iteration=20000):\n",
    "        self.adj_all = []\n",
    "        self.len_all = []\n",
    "        for G in G_list:\n",
    "            self.adj_all.append(np.asarray(nx.to_numpy_matrix(G)))\n",
    "            self.len_all.append(G.number_of_nodes())\n",
    "        if max_num_node is None:\n",
    "            self.n = max(self.len_all)\n",
    "        else:\n",
    "            self.n = max_num_node\n",
    "        if max_prev_node is None:\n",
    "            print('calculating max previous node, total iteration: {}'.format(iteration))\n",
    "            self.max_prev_node = max(self.calc_max_prev_node(iter=iteration))\n",
    "            print('max previous node: {}'.format(self.max_prev_node))\n",
    "        else:\n",
    "            self.max_prev_node = max_prev_node\n",
    "\n",
    "        # self.max_prev_node = max_prev_node\n",
    "\n",
    "        # # sort Graph in descending order\n",
    "        # len_batch_order = np.argsort(np.array(self.len_all))[::-1]\n",
    "        # self.len_all = [self.len_all[i] for i in len_batch_order]\n",
    "        # self.adj_all = [self.adj_all[i] for i in len_batch_order]\n",
    "    def __len__(self):\n",
    "        return len(self.adj_all)\n",
    "    def __getitem__(self, idx):\n",
    "        adj_copy = self.adj_all[idx].copy()\n",
    "        x_batch = np.zeros((self.n, self.max_prev_node))  # here zeros are padded for small graph\n",
    "        x_batch[0,:] = 1 # the first input token is all ones\n",
    "        y_batch = np.zeros((self.n, self.max_prev_node))  # here zeros are padded for small graph\n",
    "        # generate input x, y pairs\n",
    "        len_batch = adj_copy.shape[0]\n",
    "        x_idx = np.random.permutation(adj_copy.shape[0])\n",
    "        adj_copy = adj_copy[np.ix_(x_idx, x_idx)]\n",
    "        adj_copy_matrix = np.asmatrix(adj_copy)\n",
    "        G = nx.from_numpy_matrix(adj_copy_matrix)\n",
    "        # then do bfs in the permuted G\n",
    "        start_idx = np.random.randint(adj_copy.shape[0])\n",
    "        x_idx = np.array(bfs_seq(G, start_idx))\n",
    "        adj_copy = adj_copy[np.ix_(x_idx, x_idx)]\n",
    "        adj_encoded = encode_adj(adj_copy.copy(), max_prev_node=self.max_prev_node)\n",
    "        # get x and y and adj\n",
    "        # for small graph the rest are zero padded\n",
    "        y_batch[0:adj_encoded.shape[0], :] = adj_encoded\n",
    "        x_batch[1:adj_encoded.shape[0] + 1, :] = adj_encoded\n",
    "        return {'x':x_batch, 'y':y_batch, 'len':len_batch, 'max_prev_node': self.max_prev_node}\n",
    "\n",
    "    def calc_max_prev_node(self, iter=20000,topk=10):\n",
    "        max_prev_node = []\n",
    "        for i in range(iter):\n",
    "            if i % (iter / 5) == 0:\n",
    "                print('iter {} times'.format(i))\n",
    "            adj_idx = np.random.randint(len(self.adj_all))\n",
    "            adj_copy = self.adj_all[adj_idx].copy()\n",
    "            # print('Graph size', adj_copy.shape[0])\n",
    "            x_idx = np.random.permutation(adj_copy.shape[0])\n",
    "            adj_copy = adj_copy[np.ix_(x_idx, x_idx)]\n",
    "            adj_copy_matrix = np.asmatrix(adj_copy)\n",
    "            G = nx.from_numpy_matrix(adj_copy_matrix)\n",
    "            # then do bfs in the permuted G\n",
    "            start_idx = np.random.randint(adj_copy.shape[0])\n",
    "            x_idx = np.array(bfs_seq(G, start_idx))\n",
    "            adj_copy = adj_copy[np.ix_(x_idx, x_idx)]\n",
    "            # encode adj\n",
    "            adj_encoded = encode_adj_flexible(adj_copy.copy())\n",
    "            max_encoded_len = max([len(adj_encoded[i]) for i in range(len(adj_encoded))])\n",
    "            max_prev_node.append(max_encoded_len)\n",
    "        max_prev_node = sorted(max_prev_node)[-1*topk:]\n",
    "        return max_prev_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeo data is being used\n"
     ]
    }
   ],
   "source": [
    "graphs_whole = pd.read_pickle(\"ZeoGraphs.p\")\n",
    "unit_cell_whole = pd.read_pickle(\"ZeoUnitCells.p\")\n",
    "max_num_nodes = 432\n",
    "batch_size = 100\n",
    "batch_share = 20\n",
    "max_num_nodes_l = 3\n",
    "max_num_nodes_g = 120\n",
    "print(\"Zeo data is being used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_batch_data:\n",
    "    def __init__(self, num_pad, batch_size, batch_share):\n",
    "        \"\"\"\n",
    "        num_pad: padding size\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_share = batch_share\n",
    "        self.num_pad = num_pad\n",
    "    \n",
    "    def makeAdj(self, graphs, num_pad):\n",
    "        adj = torch.zeros(len(graphs), num_pad, num_pad)\n",
    "        for i in range(len(graphs)):\n",
    "            graph_tmp = torch.from_numpy(nx.to_numpy_array(graphs[i]))\n",
    "            adj[i, :, :] = F.pad(graph_tmp, (0, num_pad - graph_tmp.shape[1], 0, num_pad - graph_tmp.shape[0]), \"constant\", 0)\n",
    "        \n",
    "        return adj\n",
    "    \n",
    "    def makeTrain(self, dataset, unit_cell, num_per_unit_cell):\n",
    "        '''\n",
    "        dataset: list of networkx graphs\n",
    "        unit_cell: Numpy array of unit cells identifier\n",
    "        '''\n",
    "        # Number of graphs\n",
    "        num_graph = len(dataset)\n",
    "        \n",
    "        print(\"Number of graphs:\" + str(num_graph))\n",
    "        print(dataset[0])\n",
    "\n",
    "        # Number of unit cells\n",
    "        num_unit_cell = len(set(unit_cell))\n",
    "        print(\"Number of unit cells:\" + str(num_unit_cell))\n",
    "        \n",
    "        # # Detect if the training set is too large\n",
    "        # if num_train_per_unit_cell * num_unit_cell >= num_graph:\n",
    "        #     print(\"Training set is larger than the input data!\")\n",
    "        #     return\n",
    "        \n",
    "        # Extract training and testing data\n",
    "        data_train = []\n",
    "        data_test = []\n",
    "        unit_cell_train = []\n",
    "        unit_cell_test = []\n",
    "        \n",
    "        uc = list(set(unit_cell))\n",
    "        \n",
    "        for i in range(num_unit_cell):\n",
    "\n",
    "            dataset_tmp = [dataset[k] for k in np.where(np.array(unit_cell) == uc[i])[0]]\n",
    "            if i == 0:\n",
    "                print(\"Temp Dataset Size: \" + str(len(dataset_tmp)))\n",
    "                # print(dataset_tmp)\n",
    "            random.shuffle(dataset_tmp)\n",
    "            \n",
    "\n",
    "            # What is num_per_unit_cell for? is it the number of identical unit cells\n",
    "            data_train_tmp = dataset_tmp[:num_per_unit_cell]\n",
    "            data_train.append(data_train_tmp)\n",
    "            unit_cell_tmp_train = uc[i] * num_per_unit_cell\n",
    "            unit_cell_train.append(unit_cell_tmp_train)\n",
    "            \n",
    "            data_test_tmp = dataset_tmp[num_per_unit_cell:num_per_unit_cell*2]\n",
    "            data_test.append(data_test_tmp)\n",
    "            unit_cell_tmp_test = uc[i] * num_per_unit_cell\n",
    "            unit_cell_test.append(unit_cell_tmp_test)\n",
    "            \n",
    "        # Extract shared data\n",
    "        # Randomly samples n graphs from the training data, n = batch_share\n",
    "        adj_share = []\n",
    "        for i in range(num_unit_cell):\n",
    "            adj_share_tmp = random.sample(data_train[i], self.batch_share) #####\n",
    "            adj_share.append(self.makeAdj(adj_share_tmp, self.num_pad))\n",
    "        \n",
    "        print(len(adj_share))\n",
    "        print(adj_share[0].shape)\n",
    "\n",
    "        # Make batches for training data\n",
    "        batch_remain = self.batch_size - self.batch_share\n",
    "        adj_train = []\n",
    "        unit_cell_identifier_train = []\n",
    "\n",
    "        print(num_unit_cell)\n",
    "        print(num_per_unit_cell // self.batch_size)\n",
    "\n",
    "\n",
    "        # adj_train variable, takes the graph and applies zero padding\n",
    "        # Iterates through each unit cell\n",
    "        for i in range(num_unit_cell):\n",
    "            data_train_unit_cell_tmp = data_train[i]\n",
    "            print(len(data_train_unit_cell_tmp))\n",
    "            # For each unit cell, makes batches by using n samples and m shared samples, where n = batch_size - m, and m = share size\n",
    "            for j in range(num_per_unit_cell // self.batch_size):\n",
    "                adj_train_unit_cell_tmp = torch.cat((adj_share[i], self.makeAdj(data_train_unit_cell_tmp[j * batch_remain:(j+1) * batch_remain], self.num_pad)), dim = 0)\n",
    "                adj_train.append(adj_train_unit_cell_tmp)\n",
    "                unit_cell_identifier_train.append(i)\n",
    "        print(len(unit_cell_identifier_train))\n",
    "        \n",
    "        # Make batches for testing data\n",
    "        adj_test = []\n",
    "        unit_cell_identifier_test = []\n",
    "\n",
    "        print(\"Num\")\n",
    "        print(num_unit_cell)\n",
    "        for i in range(num_unit_cell):\n",
    "            data_test_unit_cell_tmp = data_test[i]\n",
    "            for j in range(num_per_unit_cell // self.batch_size):\n",
    "                adj_test_unit_cell_tmp = self.makeAdj(data_test_unit_cell_tmp[j * self.batch_size:(j+1) * self.batch_size], self.num_pad)\n",
    "                adj_test.append(adj_test_unit_cell_tmp)\n",
    "                unit_cell_identifier_test.append(i)\n",
    "        \n",
    "        # Finalize batched training and testing data\n",
    "        num_graphs_each_unit_cell = int(len(unit_cell_identifier_train) / num_unit_cell)\n",
    "        adj_train_output = []\n",
    "        adj_test_output = []\n",
    "        for i in range(num_graphs_each_unit_cell):\n",
    "            adj_unit_cell_train = []\n",
    "            adj_unit_cell_test = []\n",
    "            for j in range(num_unit_cell):\n",
    "                adj_unit_cell_train.append(adj_train[j * num_graphs_each_unit_cell + i])\n",
    "                adj_unit_cell_test.append(adj_train[j * num_graphs_each_unit_cell + i])\n",
    "            adj_train_output.append(adj_unit_cell_train)\n",
    "            adj_test_output.append(adj_unit_cell_test)\n",
    "            \n",
    "        # Finalize batched test data\n",
    "        # num_batch_test = int(len(unit_cell_identifier_train) / num_unit_cell)\n",
    "        # adj_test_output = []\n",
    "        # for i in range(num_batch_test):\n",
    "        #     adj_test_tmp = []\n",
    "        #     for j in range(num_unit_cell):\n",
    "        #         adj_test_tmp.append(self.makeAdj(graphs = data_test[j * num_batch_test + i], num_pad = self.num_pad))\n",
    "        #     adj_test_output.append(adj_test_tmp)\n",
    "        \n",
    "        return adj_train_output, adj_test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs:5400\n",
      "Graph with 8 nodes and 14 edges\n",
      "Number of unit cells:20\n",
      "Temp Dataset Size: 270\n",
      "20\n",
      "torch.Size([20, 432, 432])\n",
      "20\n",
      "0\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "0\n",
      "Num\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "graph_loader = Make_batch_data(num_pad = max_num_nodes, batch_size = batch_size, batch_share = batch_share)\n",
    "graph_train, graph_test = graph_loader.makeTrain(dataset = graphs_whole[:270*20], unit_cell = unit_cell_whole[:270*20], num_per_unit_cell = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "x = pd.read_pickle(\"ZeoGraphs.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_loader = Make_batch_data(num_pad = max_num_nodes, batch_size = batch_size, batch_share = batch_share)\n",
    "padded = graph_loader.makeAdj(x[:100*270], 432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27000, 432, 432])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Graph with 8 nodes and 14 edges\n",
      "16\n",
      "Graph with 16 nodes and 28 edges\n",
      "24\n",
      "Graph with 24 nodes and 42 edges\n",
      "32\n",
      "Graph with 32 nodes and 64 edges\n",
      "48\n",
      "Graph with 48 nodes and 96 edges\n",
      "72\n",
      "Graph with 72 nodes and 144 edges\n",
      "96\n",
      "Graph with 96 nodes and 192 edges\n",
      "144\n",
      "Graph with 144 nodes and 288 edges\n",
      "216\n",
      "Graph with 216 nodes and 432 edges\n",
      "252\n",
      "Graph with 252 nodes and 504 edges\n",
      "378\n",
      "Graph with 378 nodes and 756 edges\n",
      "432\n",
      "Graph with 432 nodes and 864 edges\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_nodes = 0\n",
    "# for i in x:\n",
    "#     if max_nodes < i.number_of_nodes():\n",
    "#         max_nodes = i.number_of_nodes()\n",
    "#         y = i\n",
    "#         print(max_nodes)\n",
    "#         print(i)\n",
    "# max_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 378 nodes and 756 edges\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZeoliteGenProject_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
