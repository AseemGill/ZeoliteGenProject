{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from arg_helper import *\n",
    "from data import *\n",
    "from data_parallel import *\n",
    "from evaluation import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from torch import distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINLayer(pl.LightningModule):\n",
    "    def __init__(self, num_feature, batch_size, eps):\n",
    "        super().__init__()\n",
    "        self.num_feature = num_feature\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.MLP_GIN = nn.Sequential(\n",
    "            nn.Linear(self.num_feature, self.num_feature),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "    def forward(self, A, X):\n",
    "        X_tmp = (1+self.eps)*X + torch.matmul(A, X)\n",
    "        X_new = self.MLP_GIN(X_tmp)\n",
    "        return X_new\n",
    "\n",
    "class GRU_plain(pl.LightningModule):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, has_input=True, has_output=False, output_size=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.has_input = has_input\n",
    "        self.has_output = has_output\n",
    "\n",
    "        if has_input:\n",
    "            self.input = nn.Linear(input_size, embedding_size)\n",
    "            self.rnn = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        if has_output:\n",
    "            self.output = nn.Sequential(\n",
    "                nn.Linear(hidden_size, embedding_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(embedding_size, output_size)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        # initialize\n",
    "\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.25)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform(param,gain=nn.init.calculate_gain('sigmoid'))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, input_raw, hidden, pack=False, input_len=None):\n",
    "        if self.has_input:\n",
    "            input = self.input(input_raw)\n",
    "            input = self.relu(input)\n",
    "        else:\n",
    "            input = input_raw\n",
    "        if pack:\n",
    "            input = pack_padded_sequence(input, input_len, batch_first=True)\n",
    "        output_raw, hidden = self.rnn(input, hidden)\n",
    "        if pack:\n",
    "            output_raw = pad_packed_sequence(output_raw, batch_first=True)[0]\n",
    "        if self.has_output:\n",
    "            output_raw = self.output(output_raw)\n",
    "        # return hidden state at each time step\n",
    "        return output_raw\n",
    "\n",
    "\n",
    "\n",
    "class GRANMixtureBernoulli(pl.LightningModule):\n",
    "    def __init__(self, config, max_num_nodes, max_num_nodes_l, max_num_nodes_g, num_cluster, num_layer, batch_size, dim_l, dim_g):\n",
    "        super().__init__()\n",
    "        self.max_num_nodes_w = max_num_nodes\n",
    "        self.num_layer_w = num_layer\n",
    "        self.batch = batch_size\n",
    "        self.num_cluster = num_cluster\n",
    "        self.hidden_dim = self.max_num_nodes_w + self.max_num_nodes_w * self.num_cluster\n",
    "\n",
    "        # Dimension of z_l and z_g\n",
    "        self.dim_zl = dim_l\n",
    "        self.dim_zg = dim_g\n",
    "\n",
    "        # Encoder     \n",
    "        ## GIN for local and global filters\n",
    "        # self.eps_l = nn.Parameter(torch.zeros(self.num_layer_w))\n",
    "        self.register_buffer(\"eps_l\", nn.Parameter(torch.zeros(self.num_layer_w)))\n",
    "\n",
    "        self.gin_l = torch.nn.ModuleList()\n",
    "        for i in range(self.num_layer_w):\n",
    "            self.gin_l.append(GINLayer(self.max_num_nodes_w, self.batch, self.eps_l[i]))\n",
    "        # self.eps_g = nn.Parameter(torch.zeros(self.num_layer_w))\n",
    "        self.register_buffer(\"eps_g\", nn.Parameter(torch.zeros(self.num_layer_w)))\n",
    "\n",
    "        self.gin_g = torch.nn.ModuleList()\n",
    "        for i in range(self.num_layer_w):\n",
    "            self.gin_g.append(GINLayer(self.max_num_nodes_w, self.batch, self.eps_g[i]))\n",
    "        \n",
    "        ## Compute mu and sigma for VAE\n",
    "        self.mu_l = nn.Sequential(\n",
    "            nn.Linear(self.max_num_nodes_w * self.num_layer_w * self.num_cluster, self.dim_zl),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.dim_zl),\n",
    "            nn.Linear(self.dim_zl, self.dim_zl))\n",
    "        self.sigma_l = nn.Sequential(\n",
    "            nn.Linear(self.max_num_nodes_w * self.num_layer_w * self.num_cluster, self.dim_zl),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.dim_zl),\n",
    "            nn.Linear(self.dim_zl, self.dim_zl),\n",
    "            nn.ReLU())\n",
    "        self.mu_g = nn.Sequential(\n",
    "            nn.Linear(self.max_num_nodes_w * self.num_layer_w, self.dim_zg),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.dim_zg),\n",
    "            nn.Linear(self.dim_zg, self.dim_zg))\n",
    "        self.sigma_g = nn.Sequential(\n",
    "            nn.Linear(self.max_num_nodes_w * self.num_layer_w, self.dim_zg),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.dim_zg),\n",
    "            nn.Linear(self.dim_zg, self.dim_zg),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        # MLP for node clustering\n",
    "        self.MLP_NodeClustering = nn.Sequential(\n",
    "            nn.Linear(self.max_num_nodes_w * self.num_layer_w, self.num_cluster),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.max_num_nodes_w),\n",
    "            nn.Softmax(dim = 1))\n",
    "        \n",
    "        # Decoder\n",
    "        # Import parameters\n",
    "        self.max_num_nodes_l = max_num_nodes_l\n",
    "        self.max_num_nodes_g = max_num_nodes_g\n",
    "        # Local\n",
    "        self.LocalPred = nn.Sequential(\n",
    "            nn.Linear(self.dim_zl, self.dim_zl),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.dim_zl, self.max_num_nodes_l**2)\n",
    "            )\n",
    "        \n",
    "        # Global\n",
    "        self.GlobalPred = nn.Sequential(\n",
    "            nn.Linear(self.dim_zg, self.dim_zg),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.dim_zg, self.max_num_nodes_g**2)\n",
    "            )\n",
    "        \n",
    "        ## Link prediction between two unit cells\n",
    "        self.AsPred = nn.Sequential(\n",
    "            nn.Linear(self.dim_zl, self.dim_zl),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.dim_zl, self.max_num_nodes_l**2)\n",
    "            )\n",
    "        \n",
    "    def ClusterAssign(self, X):\n",
    "        # print(\"Cluster Assign X.Shape: {}\".format(X.shape))\n",
    "        nodeCluster = self.MLP_NodeClustering(X)\n",
    "        nodeClusterIndex = torch.argmax(nodeCluster, dim = 2)\n",
    "        nodeRowIndex = torch.arange(0, nodeCluster.shape[1])\n",
    "        nodeClusterAssign = torch.zeros(X.shape[0], nodeCluster.shape[1], nodeCluster.shape[2])\n",
    "        nodeClusterAssign = nodeClusterAssign.type_as(X)\n",
    "        # print(\"cluster assign: \", nodeClusterAssign.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            nodeClusterAssign[i, nodeRowIndex, nodeClusterIndex[i, :]] = 1\n",
    "            \n",
    "        cluster_tmp = torch.zeros(X.shape[0], self.num_cluster, self.num_cluster)\n",
    "        nodeClusterAssign = nodeClusterAssign.type_as(X)\n",
    "        clusterDegree = torch.sum(nodeClusterAssign, dim = 1)\n",
    "        clusterDegreeInv = 1 / clusterDegree\n",
    "        clusterDegreeInv[clusterDegreeInv == float(\"inf\")] = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            cluster_tmp[i, :, :] = torch.diag(clusterDegreeInv[i, :])\n",
    "            cluster_tmp = cluster_tmp.type_as(X)\n",
    "\n",
    "        \n",
    "        nodeClusterNorm = torch.matmul(cluster_tmp, torch.transpose(nodeClusterAssign, dim0 = 1, dim1 = 2))\n",
    "        \n",
    "        return nodeClusterNorm\n",
    "    \n",
    "    def encoder(self, A, X):\n",
    "        # print(\"ENCODER\")\n",
    "        # print(A.shape[0])\n",
    "        \n",
    "        z_l = torch.zeros(A.shape[0], self.max_num_nodes_w, self.max_num_nodes_w)\n",
    "        z_l = z_l.type_as(A)\n",
    "        i = 0\n",
    "        for layer in self.gin_l:\n",
    "            X = layer(A, X)\n",
    "            z_l = torch.cat((z_l, X), dim = 2)\n",
    "            i = i + 1\n",
    "        \n",
    "        z_l = z_l[:, :, self.max_num_nodes_w:]\n",
    "        z_l = torch.matmul(self.ClusterAssign(z_l), z_l)\n",
    "        z_l = z_l.view(X.shape[0], -1)\n",
    "        z_l_mu = self.mu_l(z_l)\n",
    "        z_l_sigma = self.sigma_l(z_l)\n",
    "        \n",
    "        z_g = torch.zeros(A.shape[0], self.max_num_nodes_w, self.max_num_nodes_w)\n",
    "        z_g = z_g.type_as(A)\n",
    "        i = 0\n",
    "        for layer in self.gin_g:\n",
    "            X = layer(A, X)\n",
    "            z_g = torch.cat((z_g, X), dim = 2)\n",
    "            i = i + 1\n",
    "        \n",
    "        z_g = z_g[:, :, self.max_num_nodes_w:]\n",
    "        z_g = torch.sum(z_g, dim = 1)\n",
    "        z_g_mu = self.mu_g(z_g)\n",
    "        z_g_sigma = self.sigma_g(z_g)\n",
    "        \n",
    "        z_l_graph = z_l_mu + torch.randn(z_l_sigma.size()).type_as(z_l_mu) * torch.exp(z_l_sigma).type_as(z_l_mu)\n",
    "        z_g_graph = z_g_mu + torch.randn(z_g_sigma.size()).type_as(z_g_mu) * torch.exp(z_g_sigma).type_as(z_g_mu)\n",
    "        \n",
    "        z_sigma_graph = torch.cat((z_l_sigma, z_g_sigma), dim = 1)\n",
    "        z_mu_graph = torch.cat((z_l_mu, z_g_mu), dim = 1)\n",
    "\n",
    "        # print(\"ENCODER END\")\n",
    "\n",
    "        return z_l_graph.type_as(A), z_g_graph.type_as(A), z_l_mu.type_as(A), z_g_mu.type_as(A), z_mu_graph.type_as(A), z_sigma_graph.type_as(A)\n",
    "    \n",
    "    # Decoder process\n",
    "    def decoder(self, z_l, z_g):\n",
    "\n",
    "\n",
    "        Al = self.LocalPred(z_l).view(z_l.shape[0], self.max_num_nodes_l, -1)\n",
    "        Al = torch.sigmoid(Al)\n",
    "\n",
    "        Ag = self.GlobalPred(z_g).view(z_l.shape[0], self.max_num_nodes_g, -1)\n",
    "        Ag = torch.sigmoid(Ag)\n",
    "        n_g = Ag.shape[1]\n",
    "        Ag = Ag * (1 - torch.eye(n_g).reshape(1, n_g, -1).repeat(Ag.shape[0], 1, 1).type_as(Ag))\n",
    "\n",
    "        As = self.AsPred(z_l.view(z_l.shape[0], 1, -1)).view(z_l.shape[0], self.max_num_nodes_l, -1)\n",
    "        As = torch.sigmoid(As)\n",
    "\n",
    "        n_l = Al.shape[1]\n",
    "        \n",
    "        Al_tmp = torch.tile(Al, (n_g, n_g)).type_as(Ag)\n",
    "        Al_mask = torch.eye(n_g).reshape(1, n_g, -1).repeat(Al.shape[0], 1, 1)\n",
    "        Al_mask = Al_mask.type_as(Ag)\n",
    "        Al_mask = torch.repeat_interleave(Al_mask, n_l, dim = 1)\n",
    "        Al_mask = torch.repeat_interleave(Al_mask, n_l, dim = 2)\n",
    "        A_tmp = Al_tmp * Al_mask\n",
    "\n",
    "        As_tmp = torch.tile(As, (n_g, n_g))\n",
    "        As_tmp = As_tmp.type_as(Ag)\n",
    "        Ag_tmp = torch.repeat_interleave(Ag, n_l, dim = 1)\n",
    "        Ag_tmp = torch.tril(torch.repeat_interleave(Ag_tmp, n_l, dim = 2),-1)\n",
    "        \n",
    "        A_pred = Ag_tmp * As_tmp + torch.transpose(Ag_tmp * As_tmp, dim0=1, dim1=2).type_as(Ag) + A_tmp\n",
    "\n",
    "        # print(\"DECODER END\")\n",
    "\n",
    "        return Al, Ag, As, A_pred\n",
    "\n",
    "    # Combine encoder and decoder process\n",
    "    def vae(self, A_pad, X):\n",
    "        # encoder\n",
    "        z_l, z_g, z_l_mu, z_g_mu, z_mu_graph, z_sigma_graph = self.encoder(A_pad, X)\n",
    "        \n",
    "        # decoder\n",
    "        Al_pred, Ag_pred, As_pred, A_pred = self.decoder(z_l, z_g)\n",
    "        return z_l_mu, z_g_mu, z_mu_graph, z_sigma_graph, Al_pred, Ag_pred, As_pred, A_pred\n",
    "\n",
    "\n",
    "    def forward(self, inputgraph):\n",
    "        # print(\"FORWARD\")\n",
    "\n",
    "        # print(inputgraph)\n",
    "        # print(len(inputgraph))\n",
    "        graph = (inputgraph[0].float(),)\n",
    "        # print(inputgraph[0].shape)\n",
    "        # print(graph)\n",
    "        # Input data\n",
    "        z_l_mu = ()\n",
    "        z_g_mu = ()\n",
    "        kl_loss = 0\n",
    "        adj_loss = 0\n",
    "        A_gen = []\n",
    "\n",
    "        # print(graph.shape)\n",
    "        for i in range(len(graph)):\n",
    "            # A is the Adjaency matrix of the graph\n",
    "            # X is an identity matrix with teh same dimensions\n",
    "            A = graph[i]\n",
    "            # print(A.shape[0])\n",
    "            X = torch.eye(A.shape[1]).view(1, A.shape[1], -1).repeat(A.shape[0], 1, 1)\n",
    "            X = X.type_as(A)\n",
    "            # print(X.shape)\n",
    "            # raise Exception(\"STOP\")\n",
    "            # print(\"BEFORE VAE\\nX Shape: {}\\n A Shape: {}\\n\".format(A.shape,X.shape))\n",
    "\n",
    "            z_l_mu_tmp, z_g_mu_tmp, z_mu_graph, z_sigma_graph, Al_pred, Ag_pred, As_pred, A_pred = self.vae(A, X)\n",
    "            \n",
    "            z_l_mu = z_l_mu + (z_l_mu_tmp, )\n",
    "            z_g_mu = z_g_mu + (z_g_mu_tmp, )\n",
    "            kl_loss = kl_loss + torch.mean(-(0.5) * (1 + z_sigma_graph - z_mu_graph**2 - torch.exp(z_sigma_graph) ** 2))\n",
    "            print(A_pred.shape)\n",
    "            print(A.shape)\n",
    "            adj_loss = adj_loss + F.binary_cross_entropy(A_pred, A)\n",
    "            # print(\"KL and ADJ Loss\")\n",
    "            A_gen.append(A_pred.detach().cpu().numpy())\n",
    "            \n",
    "        # z_l_mu = torch.cat(z_l_mu, dim = 0).cpu()\n",
    "        z_g_mu = torch.cat(z_g_mu, dim = 0)\n",
    "        z_l_mu = torch.cat(z_l_mu, dim = 0)\n",
    "        T= 0.2\n",
    "        sim_matrix = torch.einsum('ik,jk->ij', z_l_mu, z_l_mu) / torch.sqrt(torch.einsum(\"i,j->ij\", torch.einsum('ij,ij->i', z_l_mu, z_l_mu), torch.einsum('ij,ij->i', z_l_mu, z_l_mu)))\n",
    "        sim_matrix = torch.exp(sim_matrix / T)\n",
    "        sim_node = torch.sum(sim_matrix, dim = 0)\n",
    "        if len(graph) > 1:\n",
    "            sim_node_tmp = sim_matrix[:X.shape[0], :X.shape[0]]\n",
    "            for j in range(len(graph) - 1):\n",
    "                sim_node_tmp = torch.cat((sim_node_tmp, sim_matrix[X.shape[0]*(j+1):X.shape[0]*(j+2), X.shape[0]*(j+1):X.shape[0]*(j+2)]), dim = 1)\n",
    "            sim_node = sim_node - torch.sum(sim_node_tmp, dim = 0)\n",
    " \n",
    "        sim_node = sim_matrix / sim_node\n",
    "        regularization = - torch.log(sim_node).mean()    \n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = 100000000*adj_loss + 1000000*kl_loss + 10000000*regularization\n",
    "        # Output\n",
    "        return total_loss, adj_loss, kl_loss, regularization, A_gen, z_l_mu, z_g_mu\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        total_loss, adj_loss, kl_loss, reg, A_tmp, zl, zg = self(*[(batch,)])\n",
    "        values = {\"total_loss\": total_loss, \"adj_loss\": adj_loss, \"kl_loss\": kl_loss, \"reg\" : reg}\n",
    "        # self.log_dict(values)\n",
    "        self.log(\"total_loss\", total_loss)\n",
    "        return total_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_nodes = 432\n",
    "batch_size = 1 # What is batch sharing?\n",
    "max_num_nodes_l = 12\n",
    "max_num_nodes_g = 36\n",
    "num_per_unit_cell = 60 # How many instances of the same unit cell graph will be included in each training and testing sample \n",
    "config = get_config(\"one-gpu.yaml\")\n",
    "model = GRANMixtureBernoulli(config = config, max_num_nodes = max_num_nodes, max_num_nodes_l = max_num_nodes_l, max_num_nodes_g = max_num_nodes_g, num_cluster = 4, num_layer = 3, batch_size = batch_size, dim_l = 512, dim_g = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,432,432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 432, 432])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([432, 432, 432])\n",
      "torch.Size([432, 432])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([432, 432])) that is different to the input size (torch.Size([432, 432, 432])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y \u001b[39m=\u001b[39m model(x)\n",
      "File \u001b[1;32mc:\\Users\\GillA\\anaconda3\\envs\\ZeoGenProject_pl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[27], line 297\u001b[0m, in \u001b[0;36mGRANMixtureBernoulli.forward\u001b[1;34m(self, inputgraph)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[39mprint\u001b[39m(A_pred\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    296\u001b[0m \u001b[39mprint\u001b[39m(A\u001b[39m.\u001b[39mshape)\n\u001b[1;32m--> 297\u001b[0m adj_loss \u001b[39m=\u001b[39m adj_loss \u001b[39m+\u001b[39m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(A_pred, A)\n\u001b[0;32m    298\u001b[0m \u001b[39m# print(\"KL and ADJ Loss\")\u001b[39;00m\n\u001b[0;32m    299\u001b[0m A_gen\u001b[39m.\u001b[39mappend(A_pred\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\GillA\\anaconda3\\envs\\ZeoGenProject_pl\\lib\\site-packages\\torch\\nn\\functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3087\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3088\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[1;32m-> 3089\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3092\u001b[0m     )\n\u001b[0;32m   3094\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([432, 432])) that is different to the input size (torch.Size([432, 432, 432])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZeoliteGenProject_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
