{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "class GINLayer(torch.nn.Module):\n",
    "    def __init__(self, num_feature, batch_size, eps):\n",
    "        super().__init__()\n",
    "        self.num_feature = num_feature\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.MLP_GIN = nn.Sequential(\n",
    "            nn.Linear(self.num_feature, self.num_feature),\n",
    "            nn.ReLU()\n",
    "            ).cuda()\n",
    "        \n",
    "    def forward(self, A, X):\n",
    "        X_tmp = (1+self.eps)*X + torch.matmul(A, X)\n",
    "        X_new = self.MLP_GIN(X_tmp)\n",
    "        return X_new\n",
    "\n",
    "class GRU_plain(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, has_input=True, has_output=False, output_size=None):\n",
    "        super(GRU_plain, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.has_input = has_input\n",
    "        self.has_output = has_output\n",
    "\n",
    "        if has_input:\n",
    "            self.input = nn.Linear(input_size, embedding_size)\n",
    "            self.rnn = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                              batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        if has_output:\n",
    "            self.output = nn.Sequential(\n",
    "                nn.Linear(hidden_size, embedding_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(embedding_size, output_size)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        # initialize\n",
    "\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.25)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform(param,gain=nn.init.calculate_gain('sigmoid'))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, input_raw, hidden, pack=False, input_len=None):\n",
    "        if self.has_input:\n",
    "            input = self.input(input_raw)\n",
    "            input = self.relu(input)\n",
    "        else:\n",
    "            input = input_raw\n",
    "        if pack:\n",
    "            input = pack_padded_sequence(input, input_len, batch_first=True)\n",
    "        output_raw, hidden = self.rnn(input, hidden)\n",
    "        if pack:\n",
    "            output_raw = pad_packed_sequence(output_raw, batch_first=True)[0]\n",
    "        if self.has_output:\n",
    "            output_raw = self.output(output_raw)\n",
    "        # return hidden state at each time step\n",
    "        return output_raw.cuda()\n",
    "\n",
    "class GRANMixtureBernoulli(nn.Module):\n",
    "    def __init__(self, config, max_num_nodes, max_num_nodes_l, max_num_nodes_g, num_cluster, num_layer, batch_size, dim_l, dim_g):\n",
    "        super(GRANMixtureBernoulli, self).__init__()\n",
    "        self.max_num_nodes_w = max_num_nodes\n",
    "        self.num_layer_w = num_layer\n",
    "        self.batch = batch_size\n",
    "        self.num_cluster = num_cluster\n",
    "        self.hidden_dim = self.max_num_nodes_w + self.max_num_nodes_w * self.num_cluster\n",
    "        \n",
    "        # Dimension of z_l and z_g\n",
    "        self.dim_zl = dim_l\n",
    "        self.dim_zg = dim_g\n",
    "\n",
    "        # Encoder     \n",
    "        ## GIN for local and global filters\n",
    "        self.eps_l = nn.Parameter(torch.zeros(self.num_layer_w)).cuda()\n",
    "        self.gin_l = torch.nn.ModuleList()\n",
    "        for i in range(self.num_layer_w):\n",
    "            self.gin_l.append(GINLayer(self.max_num_nodes_w, self.batch, self.eps_l[i]))\n",
    "        self.eps_g = nn.Parameter(torch.zeros(self.num_layer_w)).cuda()\n",
    "        self.gin_g = torch.nn.ModuleList()\n",
    "        for i in range(self.num_layer_w):\n",
    "            self.gin_g.append(GINLayer(self.max_num_nodes_w, self.batch, self.eps_g[i]))\n",
    "        \n",
    "        ## Compute mu and sigma for VAE\n",
    "        self.mu_l = nn.Sequential(\n",
    "            nn.Linear(self.max_num_nodes_w * self.num_layer_w * self.num_cluster, self.dim_zl),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.dim_zl),\n",
    "            nn.Linear(self.dim_zl, self.dim_zl))\n",
    "        self.sigma_l = nn.Sequential(\n",
    "            nn.Linear(self.max_num_nodes_w * self.num_layer_w * self.num_cluster, self.dim_zl),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.dim_zl),\n",
    "            nn.Linear(self.dim_zl, self.dim_zl),\n",
    "            nn.ReLU())\n",
    "        self.mu_g = nn.Sequential(\n",
    "            nn.Linear(self.max_num_nodes_w * self.num_layer_w, self.dim_zg),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.dim_zg),\n",
    "            nn.Linear(self.dim_zg, self.dim_zg))\n",
    "        self.sigma_g = nn.Sequential(\n",
    "            nn.Linear(self.max_num_nodes_w * self.num_layer_w, self.dim_zg),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.dim_zg),\n",
    "            nn.Linear(self.dim_zg, self.dim_zg),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        # MLP for node clustering\n",
    "        self.MLP_NodeClustering = nn.Sequential(\n",
    "            nn.Linear(self.max_num_nodes_w * self.num_layer_w, self.num_cluster),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.max_num_nodes_w),\n",
    "            nn.Softmax(dim = 1)).cuda()\n",
    "        \n",
    "        # Decoder\n",
    "        # Import parameters\n",
    "        self.max_num_nodes_l = max_num_nodes_l\n",
    "        self.max_num_nodes_g = max_num_nodes_g\n",
    "        # Local\n",
    "        self.LocalPred = nn.Sequential(\n",
    "            nn.Linear(self.dim_zl, self.dim_zl),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.dim_zl, self.max_num_nodes_l**2)\n",
    "            )\n",
    "        \n",
    "        # Global\n",
    "        self.GlobalPred = nn.Sequential(\n",
    "            nn.Linear(self.dim_zg, self.dim_zg),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.dim_zg, self.max_num_nodes_g**2)\n",
    "            )\n",
    "        \n",
    "        ## Link prediction between two unit cells\n",
    "        self.AsPred = nn.Sequential(\n",
    "            nn.Linear(self.dim_zl, self.dim_zl),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.dim_zl, self.max_num_nodes_l**2)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def ClusterAssign(self, X):\n",
    "        nodeCluster = self.MLP_NodeClustering(X)\n",
    "        nodeClusterIndex = torch.argmax(nodeCluster, dim = 2)\n",
    "        nodeRowIndex = torch.arange(0, nodeCluster.shape[1])\n",
    "        nodeClusterAssign = torch.zeros(self.batch, nodeCluster.shape[1], nodeCluster.shape[2])\n",
    "        # print(\"cluster assign: \", nodeClusterAssign.shape)\n",
    "        for i in range(self.batch):\n",
    "            nodeClusterAssign[i, nodeRowIndex, nodeClusterIndex[i, :]] = 1\n",
    "            \n",
    "        cluster_tmp = torch.zeros(self.batch, self.num_cluster, self.num_cluster)\n",
    "        clusterDegree = torch.sum(nodeClusterAssign, dim = 1)\n",
    "        clusterDegreeInv = 1 / clusterDegree\n",
    "        clusterDegreeInv[clusterDegreeInv == float(\"inf\")] = 0\n",
    "        for i in range(self.batch):\n",
    "            cluster_tmp[i, :, :] = torch.diag(clusterDegreeInv[i, :])\n",
    "        \n",
    "        nodeClusterNorm = torch.matmul(cluster_tmp, torch.transpose(nodeClusterAssign, dim0 = 1, dim1 = 2))\n",
    "        \n",
    "        return nodeClusterNorm\n",
    "    \n",
    "    def encoder(self, A, X):\n",
    "        \n",
    "        z_l = torch.zeros(self.batch, self.max_num_nodes_w, self.max_num_nodes_w).cuda()\n",
    "        i = 0\n",
    "        for layer in self.gin_l:\n",
    "            X = layer(A, X)\n",
    "            z_l = torch.cat((z_l, X), dim = 2)\n",
    "            i = i + 1\n",
    "        \n",
    "        z_l = z_l[:, :, self.max_num_nodes_w:].cuda()\n",
    "        z_l = torch.matmul(self.ClusterAssign(z_l).cuda(), z_l)\n",
    "        z_l = z_l.view(self.batch, -1)\n",
    "        z_l_mu = self.mu_l(z_l)\n",
    "        z_l_sigma = self.sigma_l(z_l)\n",
    "        \n",
    "        z_g = torch.zeros(self.batch, self.max_num_nodes_w, self.max_num_nodes_w).cuda()\n",
    "        i = 0\n",
    "        for layer in self.gin_g:\n",
    "            X = layer(A, X)\n",
    "            z_g = torch.cat((z_g, X), dim = 2)\n",
    "            i = i + 1\n",
    "        \n",
    "        z_g = z_g[:, :, self.max_num_nodes_w:].cuda()\n",
    "        z_g = torch.sum(z_g, dim = 1)\n",
    "        z_g_mu = self.mu_g(z_g)\n",
    "        z_g_sigma = self.sigma_g(z_g)\n",
    "        \n",
    "        z_l_graph = z_l_mu + torch.randn(z_l_sigma.size()).cuda() * torch.exp(z_l_sigma)\n",
    "        z_g_graph = z_g_mu + torch.randn(z_g_sigma.size()).cuda() * torch.exp(z_g_sigma)\n",
    "        \n",
    "        z_sigma_graph = torch.cat((z_l_sigma, z_g_sigma), dim = 1)\n",
    "        z_mu_graph = torch.cat((z_l_mu, z_g_mu), dim = 1)\n",
    "\n",
    "        return z_l_graph.cuda(), z_g_graph.cuda(), z_l_mu.cuda(), z_g_mu.cuda(), z_mu_graph.cuda(), z_sigma_graph.cuda()\n",
    "    \n",
    "    # Decoder process\n",
    "    def decoder(self, z_l, z_g):\n",
    "\n",
    "        Al = self.LocalPred(z_l).cuda().view(self.batch, self.max_num_nodes_l, -1).cuda()\n",
    "        Al = torch.sigmoid(Al).cuda()\n",
    "\n",
    "        Ag = self.GlobalPred(z_g).cuda().view(self.batch, self.max_num_nodes_g, -1).cuda()\n",
    "        Ag = torch.sigmoid(Ag).cuda()\n",
    "        n_g = Ag.shape[1]\n",
    "        Ag = Ag * (1 - torch.eye(n_g).reshape(1, n_g, -1).repeat(Ag.shape[0], 1, 1).cuda())\n",
    "\n",
    "        As = self.AsPred(z_l.view(self.batch, 1, -1)).view(self.batch, self.max_num_nodes_l, -1).cuda()\n",
    "        As = torch.sigmoid(As).cuda()\n",
    "\n",
    "        n_l = Al.shape[1]\n",
    "        \n",
    "        Al_tmp = torch.tile(Al, (n_g, n_g))\n",
    "        Al_mask = torch.eye(n_g).reshape(1, n_g, -1).repeat(Al.shape[0], 1, 1).cuda()\n",
    "        Al_mask = torch.repeat_interleave(Al_mask, n_l, dim = 1)\n",
    "        Al_mask = torch.repeat_interleave(Al_mask, n_l, dim = 2)\n",
    "        A_tmp = Al_tmp * Al_mask\n",
    "\n",
    "        As_tmp = torch.tile(As, (n_g, n_g))\n",
    "        Ag_tmp = torch.repeat_interleave(Ag, n_l, dim = 1)\n",
    "        Ag_tmp = torch.tril(torch.repeat_interleave(Ag_tmp, n_l, dim = 2),-1)\n",
    "        \n",
    "        A_pred = Ag_tmp * As_tmp + torch.transpose(Ag_tmp * As_tmp, dim0=1, dim1=2) + A_tmp\n",
    "        \n",
    "        return Al, Ag, As, A_pred\n",
    "\n",
    "    # Combine encoder and decoder process\n",
    "    def vae(self, A_pad, X):\n",
    "        # encoder\n",
    "        z_l, z_g, z_l_mu, z_g_mu, z_mu_graph, z_sigma_graph = self.encoder(A_pad, X)\n",
    "        \n",
    "        # decoder\n",
    "        Al_pred, Ag_pred, As_pred, A_pred = self.decoder(z_l, z_g)\n",
    "        return z_l_mu, z_g_mu, z_mu_graph, z_sigma_graph, Al_pred, Ag_pred, As_pred, A_pred\n",
    "\n",
    "\n",
    "    def forward(self, inputgraph):\n",
    "        \n",
    "        graph = inputgraph\n",
    "        # Input data\n",
    "        z_l_mu = ()\n",
    "        z_g_mu = ()\n",
    "        kl_loss = 0\n",
    "        adj_loss = 0\n",
    "        A_gen = []\n",
    "        for i in range(len(graph)):\n",
    "            A = graph[i].cuda()\n",
    "            X = torch.eye(A.shape[1]).view(1, A.shape[1], -1).repeat(A.shape[0], 1, 1).cuda()\n",
    "            z_l_mu_tmp, z_g_mu_tmp, z_mu_graph, z_sigma_graph, Al_pred, Ag_pred, As_pred, A_pred = self.vae(A, X)\n",
    "            \n",
    "            z_l_mu = z_l_mu + (z_l_mu_tmp, )\n",
    "            z_g_mu = z_g_mu + (z_g_mu_tmp, )\n",
    "            kl_loss = kl_loss + torch.mean(-(0.5) * (1 + z_sigma_graph - z_mu_graph**2 - torch.exp(z_sigma_graph) ** 2))\n",
    "            adj_loss = adj_loss + F.binary_cross_entropy(A_pred, A)\n",
    "            A_gen.append(A_pred.detach().cpu().numpy())\n",
    "            \n",
    "        # z_l_mu = torch.cat(z_l_mu, dim = 0).cpu()\n",
    "        z_g_mu = torch.cat(z_g_mu, dim = 0)\n",
    "        z_l_mu = torch.cat(z_l_mu, dim = 0)\n",
    "        T= 0.2\n",
    "        sim_matrix = torch.einsum('ik,jk->ij', z_l_mu, z_l_mu) / torch.sqrt(torch.einsum(\"i,j->ij\", torch.einsum('ij,ij->i', z_l_mu, z_l_mu), torch.einsum('ij,ij->i', z_l_mu, z_l_mu)))\n",
    "        sim_matrix = torch.exp(sim_matrix / T)\n",
    "        sim_node = torch.sum(sim_matrix, dim = 0)\n",
    "        if len(graph) > 1:\n",
    "            sim_node_tmp = sim_matrix[:self.batch, :self.batch]\n",
    "            for j in range(len(graph) - 1):\n",
    "                sim_node_tmp = torch.cat((sim_node_tmp, sim_matrix[self.batch*(j+1):self.batch*(j+2), self.batch*(j+1):self.batch*(j+2)]), dim = 1)\n",
    "            sim_node = sim_node - torch.sum(sim_node_tmp, dim = 0)\n",
    " \n",
    "        sim_node = sim_matrix / sim_node\n",
    "        regularization = - torch.log(sim_node).mean().cuda()            \n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = 100000000*adj_loss.cuda() + 1000000*kl_loss.cuda() + 10000000*regularization\n",
    "        # Output\n",
    "        return total_loss, adj_loss, kl_loss, regularization, A_gen, z_l_mu, z_g_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(\"gran_grid.yaml\")\n",
    "\n",
    "\n",
    "model = GRANMixtureBernoulli(config = config, max_num_nodes = max_num_nodes, max_num_nodes_l = max_num_nodes_l, max_num_nodes_g = max_num_nodes_g, num_cluster = 4, num_layer = 3, batch_size = batch_size, dim_l = 512, dim_g = 512)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZeoliteGenProject_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
